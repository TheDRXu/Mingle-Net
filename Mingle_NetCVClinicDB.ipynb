{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_tlZ3i7w3ud",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import glob\n",
        "import cv2\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import pickle\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Reshape, concatenate, Concatenate, Conv2D, Conv2DTranspose, MaxPooling2D, UpSampling2D\n",
        "from tensorflow.keras.layers import Input, Activation, BatchNormalization, Dropout, Lambda, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate\n",
        "from tensorflow.keras.models import Model, load_model, save_model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping,TensorBoard,LearningRateScheduler\n",
        "\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from skimage.color import rgb2gray as rtg\n",
        "from skimage.io import imread, imshow\n",
        "from skimage.transform import resize\n",
        "from PIL import Image\n",
        "from datetime import datetime\n",
        "import time\n",
        "from keras.callbacks import CSVLogger\n",
        "from deeplab import deeplabModel\n",
        "from deeplabv1 import deeplab\n",
        "from unet import unet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbNQZpqOoyJY"
      },
      "source": [
        "# PreProcessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sawusfw8yd6l"
      },
      "outputs": [],
      "source": [
        "def load_data(img_height, img_width, images_to_be_loaded):\n",
        "    IMAGES_PATH = \"CVC-ClinicDB/TIF/Original/\"\n",
        "    MASKS_PATH = \"CVC-ClinicDB/TIF/Ground Truth/\"\n",
        "\n",
        "    train_ids = glob(IMAGES_PATH + \"*.tif\")\n",
        "\n",
        "    if images_to_be_loaded == -1:\n",
        "        images_to_be_loaded = len(train_ids)\n",
        "\n",
        "    X_train = np.zeros((images_to_be_loaded, img_height, img_width, 3), dtype=np.float32)\n",
        "    Y_train = np.zeros((images_to_be_loaded, img_height, img_width), dtype=np.uint8)\n",
        "\n",
        "    print('Resizing training images and masks: ' + str(images_to_be_loaded))\n",
        "    for n, id_ in tqdm(enumerate(train_ids)):\n",
        "        if n == images_to_be_loaded:\n",
        "            break\n",
        "\n",
        "        image_path = id_\n",
        "        mask_path = image_path.replace(\"Original\", \"Ground Truth\")\n",
        "\n",
        "        image = imread(image_path)\n",
        "        mask_ = imread(mask_path)\n",
        "\n",
        "\n",
        "        mask = np.zeros((img_height, img_width), dtype=np.bool_)\n",
        "\n",
        "        pillow_image = Image.fromarray(image)\n",
        "\n",
        "        pillow_image = pillow_image.resize((img_height, img_width))\n",
        "        image = np.array(pillow_image)\n",
        "\n",
        "        X_train[n] = image / 255\n",
        "\n",
        "        pillow_mask = Image.fromarray(mask_)\n",
        "        pillow_mask = pillow_mask.resize((img_height, img_width), resample=Image.LANCZOS)\n",
        "        mask_ = np.array(pillow_mask)\n",
        "\n",
        "        for i in range(img_height):\n",
        "            for j in range(img_width):\n",
        "                if (mask_[i, j] >= 127).any():\n",
        "                    mask[i, j] = 1\n",
        "\n",
        "        Y_train[n] = mask\n",
        "\n",
        "    Y_train = np.expand_dims(Y_train, axis=-1)\n",
        "\n",
        "    return X_train, Y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaCLlZZSP247"
      },
      "outputs": [],
      "source": [
        "img_size = 256\n",
        "X, Y = load_data(img_size, img_size, -1) #Resize (352,352) for Kvasir-SEG & CVC-ColonDB while (256,256) for CVC-ClinicDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sosaSqupU6qQ"
      },
      "outputs": [],
      "source": [
        "test_size = 0.2\n",
        "validation_size = 0.5\n",
        "\n",
        "#Split the the data into train:validation:test with 8:1:1 Ratio\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=0)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=validation_size, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "hLOVsS0rP247"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "for i in range(0,5):\n",
        "    image_x = random.randint(0, 60)\n",
        "\n",
        "    plt.subplot(1,2,1)\n",
        "    imshow(X_val[image_x])\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    imshow(y_val[image_x], cmap='gray')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Augmentation"
      ],
      "metadata": {
        "id": "jC7EyxOMQawr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAlJf6snzg_5"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "from albumentations.augmentations import functional as F\n",
        "\n",
        "aug_train = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.ColorJitter(brightness=(0.6,1.6), contrast=0.2, saturation=0.1, hue=0.01, always_apply=True),\n",
        "    A.Affine(scale=(0.5,1.5), translate_percent=(-0.125,0.125), rotate=(-180,180), shear=(-22.5,22), always_apply=True),\n",
        "    A.GaussianBlur(blur_limit=(25, 25),sigma_limit=(0.001, 2.0),always_apply=False, p=0.5),\n",
        "])\n",
        "\n",
        "def augment_images():\n",
        "    x_train_out = []\n",
        "    y_train_out = []\n",
        "\n",
        "    for i in range (len(X_train)):\n",
        "        ug = aug_train(image=X_train[i], mask=y_train[i])\n",
        "        x_train_out.append(ug['image'])\n",
        "        y_train_out.append(ug['mask'])\n",
        "\n",
        "    return np.array(x_train_out), np.array(y_train_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Metrics"
      ],
      "metadata": {
        "id": "rtcUK34AQgjF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23SX_iGf3B6p"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "import numpy as np\n",
        "def dice_coeff(y_true, y_pred):\n",
        "\n",
        "    _epsilon = 10 ** -7\n",
        "    intersections = tf.reduce_sum(y_true * y_pred)\n",
        "    unions = tf.reduce_sum(y_true + y_pred)\n",
        "    dice_scores = (2.0 * intersections + _epsilon) / (unions + _epsilon)\n",
        "\n",
        "    return dice_scores\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "\n",
        "    loss = 1 - dice_coeff(y_true, y_pred)\n",
        "\n",
        "    return loss\n",
        "\n",
        "def total_loss(y_true, y_pred):\n",
        "    return 0.5*binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "\n",
        "def IoU(y_true, y_pred, eps=1e-6):\n",
        "\n",
        "    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
        "    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3]) - intersection\n",
        "\n",
        "    return K.mean( (intersection + eps) / (union + eps), axis=0)\n",
        "\n",
        "def zero_IoU(y_true, y_pred):\n",
        "\n",
        "    return IoU(1-y_true, 1-y_pred)\n",
        "\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "\n",
        "    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "\n",
        "def tversky(y_true, y_pred, smooth=1, alpha=0.7):\n",
        "\n",
        "    y_true_pos = tf.reshape(y_true,[-1])\n",
        "    y_pred_pos = tf.reshape(y_pred,[-1])\n",
        "    true_pos = tf.reduce_sum(y_true_pos * y_pred_pos)\n",
        "    false_neg = tf.reduce_sum(y_true_pos * (1 - y_pred_pos))\n",
        "    false_pos = tf.reduce_sum((1 - y_true_pos) * y_pred_pos)\n",
        "\n",
        "    return (true_pos + smooth) / (true_pos + alpha * false_neg + (1 - alpha) * false_pos + smooth)\n",
        "\n",
        "\n",
        "def tversky_loss(y_true, y_pred):\n",
        "\n",
        "    return 1 - tversky(y_true, y_pred)\n",
        "\n",
        "def focal_tversky_loss(y_true, y_pred, gamma=0.75):\n",
        "\n",
        "    tv = tversky(y_true, y_pred)\n",
        "\n",
        "    return K.pow((1 - tv), gamma)\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(tf.math.round(y_pred), tf.float32)\n",
        "\n",
        "    tp = tf.reduce_sum(y_true * y_pred)\n",
        "    fp = tf.reduce_sum((1 - y_true) * y_pred)\n",
        "    fn = tf.reduce_sum(y_true * (1 - y_pred))\n",
        "\n",
        "    precision = tp / (tp + fp + tf.keras.backend.epsilon())\n",
        "    recall = tp / (tp + fn + tf.keras.backend.epsilon())\n",
        "\n",
        "    f1 = 2 * precision * recall / (precision + recall + tf.keras.backend.epsilon())\n",
        "    return f1\n",
        "\n",
        "def dice_metric_loss(ground_truth, predictions, smooth=1e-6):\n",
        "    ground_truth = K.cast(ground_truth, tf.float32)\n",
        "    predictions = K.cast(predictions, tf.float32)\n",
        "    ground_truth = K.flatten(ground_truth)\n",
        "    predictions = K.flatten(predictions)\n",
        "    intersection = K.sum(predictions * ground_truth)\n",
        "    union = K.sum(predictions) + K.sum(ground_truth)\n",
        "\n",
        "    dice = (2. * intersection + smooth) / (union + smooth)\n",
        "\n",
        "    return 1 - dice"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layer 1 (Deeplabv3+ and DoubleUNet)"
      ],
      "metadata": {
        "id": "dK25F3kRQlDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Deeplabv3+ Layer 1"
      ],
      "metadata": {
        "id": "EZc3w2cTQp2n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtd1TetLP248"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
        "\n",
        "from keras.layers import (\n",
        "    Conv2D,\n",
        "    BatchNormalization,\n",
        "    Activation,\n",
        "    Concatenate,\n",
        "    Input,\n",
        "    Dropout,\n",
        ")\n",
        "from keras.layers import (\n",
        "    AveragePooling2D,\n",
        "    GlobalAveragePooling2D,\n",
        "    UpSampling2D,\n",
        "    Reshape,\n",
        "    Dense,\n",
        ")\n",
        "from keras.models import Model\n",
        "from keras.applications import ResNet50, ResNet101\n",
        "from keras.regularizers import l2\n",
        "from tensorflow.python.keras.engine.keras_tensor import KerasTensor\n",
        "\n",
        "\n",
        "def squeeze_and_excite(inputs: KerasTensor, ratio: int = 8) -> KerasTensor:\n",
        "    \"\"\"Function to apply Squeeze & Excitation to a feature map.\n",
        "\n",
        "    Args:\n",
        "        inputs (KerasTensor): Feature Map\n",
        "        ratio (int, optional): Ratio for excitation in first dense layer. Defaults to 8.\n",
        "\n",
        "    Returns:\n",
        "        KerasTensor: Re-calibrated feature map.\n",
        "    \"\"\"\n",
        "    init = inputs\n",
        "    filters = init.shape[-1]\n",
        "    se_shape = (1, 1, filters)\n",
        "\n",
        "    se = GlobalAveragePooling2D()(init)\n",
        "    se = Reshape(se_shape)(se)\n",
        "    se = Dense(\n",
        "        filters // ratio,\n",
        "        activation=\"relu\",\n",
        "        kernel_initializer=\"he_normal\",\n",
        "        use_bias=False,\n",
        "    )(se)\n",
        "    se = Dense(\n",
        "        filters,\n",
        "        activation=\"sigmoid\",\n",
        "        kernel_initializer=\"he_normal\",\n",
        "        use_bias=False,\n",
        "    )(se)\n",
        "\n",
        "    x = init * se\n",
        "    return x\n",
        "\n",
        "\n",
        "def ASPP(inputs: KerasTensor) -> KerasTensor:\n",
        "    \"\"\"Function to apply Atrous Spatial Pyramid Pooling on features from backbone.\n",
        "\n",
        "    Args:\n",
        "        inputs (KerasTensor): Features from backbone.\n",
        "\n",
        "    Returns:\n",
        "        KerasTensor: Features with better spatial context.\n",
        "    \"\"\"\n",
        "    shape = inputs.shape\n",
        "    y1 = AveragePooling2D(pool_size=(shape[1], shape[2]))(inputs)\n",
        "    y1 = Conv2D(256, 1, padding=\"same\", use_bias=False, kernel_initializer=\"he_normal\")(y1)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "    y1 = Activation(\"relu\")(y1)\n",
        "    y1 = UpSampling2D((shape[1], shape[2]), interpolation=\"bilinear\")(y1)\n",
        "\n",
        "    # 1x1 Convolution\n",
        "    y2 = Conv2D(256, 1, padding=\"same\", use_bias=False, kernel_initializer=\"he_normal\")(inputs)\n",
        "    y2 = BatchNormalization()(y2)\n",
        "    # y2 = Dropout(0.5)(y2)\n",
        "    y2 = Activation(\"relu\")(y2)\n",
        "\n",
        "    # 3x3 Convolution, Dilation Rate - 12 or 6\n",
        "    y3 = Conv2D(\n",
        "        256, 3, padding=\"same\", dilation_rate=6, use_bias=False, kernel_initializer=\"he_normal\"\n",
        "    )(inputs)\n",
        "    y3 = BatchNormalization()(y3)\n",
        "    y3 = Activation(\"relu\")(y3)\n",
        "\n",
        "    # 3x3 Convolution, Dilation Rate - 24 or 12\n",
        "    y4 = Conv2D(\n",
        "        256, 3, padding=\"same\", dilation_rate=12, use_bias=False, kernel_initializer=\"he_normal\"\n",
        "    )(inputs)\n",
        "    y4 = BatchNormalization()(y4)\n",
        "    y4 = Activation(\"relu\")(y4)\n",
        "\n",
        "    # 3x3 Convolution, Dilation Rate - 36 or 18\n",
        "    y5 = Conv2D(\n",
        "        256, 3, padding=\"same\", dilation_rate=18, use_bias=False, kernel_initializer=\"he_normal\"\n",
        "    )(inputs)\n",
        "    y5 = BatchNormalization()(y5)\n",
        "    y5 = Activation(\"relu\")(y5)\n",
        "\n",
        "    # 1x1 Convolution on the concatenated Feature Map\n",
        "    y = Concatenate()([y1, y2, y3, y4, y5])\n",
        "    y = Conv2D(256, 1, padding=\"same\", use_bias=False, kernel_initializer=\"he_normal\")(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation(\"relu\")(y)\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "def createModel(modelType: str, shape: tuple[int] = (256, 256, 3)) -> Model:\n",
        "    \"\"\"Creates a Model with the specified backbone.\n",
        "\n",
        "    Args:\n",
        "        modelType (str): Choice of backbone. ResNet50 or ResNet101.\n",
        "        shape (tuple[int]): Shape of input to the model. Defaults to (256, 256, 3).\n",
        "\n",
        "    Returns:\n",
        "        Model: Your DeepLabV3+ Model.\n",
        "    \"\"\"\n",
        "    inputs = Input(shape)  # instantiating a tensor\n",
        "\n",
        "    encoder = (\n",
        "        ResNet101(weights=\"imagenet\", include_top=False, input_tensor=inputs)\n",
        "        if modelType == \"ResNet101\"\n",
        "        else ResNet50(weights=\"imagenet\", include_top=False, input_tensor=inputs)\n",
        "    )\n",
        "\n",
        "    image_features = encoder.get_layer(\n",
        "        \"conv4_block23_out\" if modelType == \"ResNet101\" else \"conv4_block6_out\"\n",
        "    ).output\n",
        "\n",
        "    # High-Level Features\n",
        "    x_a = ASPP(image_features)\n",
        "    # Up-Sampling High-Level Features by 4\n",
        "    x_a = UpSampling2D((4, 4), interpolation=\"bilinear\")(x_a)\n",
        "    x_a = Dropout(0.5)(x_a)\n",
        "\n",
        "    # Low-Level Features\n",
        "    x_b = encoder.get_layer(\"conv2_block2_out\").output\n",
        "\n",
        "    # 1x1 Convolution on Low-Level Features\n",
        "    x_b = Conv2D(\n",
        "        filters=48, kernel_size=1, padding=\"same\", use_bias=False, kernel_initializer=\"he_normal\"\n",
        "    )(x_b)\n",
        "    x_b = BatchNormalization()(x_b)\n",
        "    x_b = Activation(\"relu\")(x_b)\n",
        "\n",
        "    # Concatenating High-Level and Low-Level Features\n",
        "    x = Concatenate()([x_a, x_b])\n",
        "    x = Dropout(0.5)(x)\n",
        "    # x = squeeze_and_excite(x)\n",
        "\n",
        "    # 3x3 Convolution on Concatenated Map\n",
        "    x = Conv2D(\n",
        "        filters=256, kernel_size=3, padding=\"same\", use_bias=False, kernel_initializer=\"he_normal\"\n",
        "    )(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = squeeze_and_excite(x)\n",
        "\n",
        "    # 3x3 Convolution on Concatenated Map\n",
        "    x = Conv2D(\n",
        "        filters=256, kernel_size=3, padding=\"same\", use_bias=False, kernel_initializer=\"he_normal\"\n",
        "    )(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    # x = Dropout(0.5)(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = squeeze_and_excite(x)\n",
        "\n",
        "    # Final Up-Sampling by 4\n",
        "    x = UpSampling2D((4, 4), interpolation=\"bilinear\")(x)\n",
        "    x = Conv2D(1, 1)(x)\n",
        "    x = Activation(\"sigmoid\")(x)\n",
        "\n",
        "    model = Model(inputs, x)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_YPvCtCP248"
      },
      "outputs": [],
      "source": [
        "seed_value = 58800\n",
        "loss = 'binary_crossentropy'\n",
        "ct = datetime.now()\n",
        "EPOCHS = 600\n",
        "dataset_type=\"cvcclinicdb\"\n",
        "model_type = \"deeplabv3p\"\n",
        "\n",
        "model_path = 'ModelSaveTensorFlow/' + dataset_type + '_' + model_type + 'newaug'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeCsLvENP248"
      },
      "outputs": [],
      "source": [
        "from keras.optimizers import AdamW\n",
        "\n",
        "deeplabv3p_model = createModel(modelType=\"ResNet101\",shape=(256,256,3))\n",
        "optimizer = AdamW(\n",
        "    learning_rate = 0.00001, weight_decay = 0.004)\n",
        "loss = 'binary_crossentropy'\n",
        "deeplabv3p_model.compile(optimizer=optimizer, loss=loss,\n",
        "                     metrics=['accuracy',f1_score,dice_coeff,dice_loss, total_loss, IoU, zero_IoU, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GWEPUmwP248"
      },
      "outputs": [],
      "source": [
        "step = 0\n",
        "learning_rate = 1e-5\n",
        "min_loss_for_saving = 0.2\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(0, EPOCHS):\n",
        "\n",
        "    print(f'Training, epoch {epoch}')\n",
        "    print('Learning Rate: ' + str(learning_rate))\n",
        "\n",
        "    step += 1\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    image_augmented, mask_augmented = augment_images()\n",
        "\n",
        "    deeplabv3p_model.fit(x=image_augmented, y=mask_augmented, epochs=1, batch_size=16, validation_data=(X_val, y_val), verbose=1)\n",
        "\n",
        "    prediction_valid = deeplabv3p_model.predict(X_val, verbose=1, batch_size=16)\n",
        "    loss_valid = dice_metric_loss(y_val, prediction_valid)\n",
        "    loss_valid = loss_valid.numpy()\n",
        "    print(\"Loss Validation: \" + str(loss_valid))\n",
        "\n",
        "    prediction_test = deeplabv3p_model.predict(X_test, verbose=0, batch_size=16)\n",
        "    loss_test = dice_metric_loss(y_test, prediction_test)\n",
        "    loss_test = loss_test.numpy()\n",
        "    print(\"Loss Test: \" + str(loss_test))\n",
        "\n",
        "    if min_loss_for_saving > loss_valid:\n",
        "        min_loss_for_saving = loss_valid\n",
        "        print(\"Saved model with val_loss: \", loss_valid)\n",
        "        deeplabv3p_model.save(model_path)\n",
        "\n",
        "    del image_augmented\n",
        "    del mask_augmented\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.compat.v1.reset_default_graph()\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "hours, rem = divmod(elapsed_time, 3600)\n",
        "minutes, seconds = divmod(rem, 60)\n",
        "print(f\"Elapsed Time: {int(hours)}h {int(minutes)}m {int(seconds)}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dq5WUsaWP249"
      },
      "outputs": [],
      "source": [
        "del deeplabv3p_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train DoubleUNet Layer 1"
      ],
      "metadata": {
        "id": "FBgKIKGARDRF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5smZGdocP249"
      },
      "outputs": [],
      "source": [
        "def f1_score(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(tf.math.round(y_pred), tf.float32)\n",
        "\n",
        "    tp = tf.reduce_sum(y_true * y_pred)\n",
        "    fp = tf.reduce_sum((1 - y_true) * y_pred)\n",
        "    fn = tf.reduce_sum(y_true * (1 - y_pred))\n",
        "\n",
        "    precision = tp / (tp + fp + tf.keras.backend.epsilon())\n",
        "    recall = tp / (tp + fn + tf.keras.backend.epsilon())\n",
        "\n",
        "    f1 = 2 * precision * recall / (precision + recall + tf.keras.backend.epsilon())\n",
        "    return f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZEPaIA-P249"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import *\n",
        "\n",
        "def squeeze_excite_block(inputs, ratio=8):\n",
        "    init = inputs\n",
        "    channel_axis = -1\n",
        "    filters = init.shape[channel_axis]\n",
        "    se_shape = (1, 1, filters)\n",
        "\n",
        "    se = GlobalAveragePooling2D()(init)\n",
        "    se = Reshape(se_shape)(se)\n",
        "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "\n",
        "    x = Multiply()([init, se])\n",
        "    return x\n",
        "\n",
        "def conv_block(inputs, filters):\n",
        "    x = inputs\n",
        "\n",
        "    x = Conv2D(filters, (3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters, (3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = squeeze_excite_block(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def encoder1(inputs):\n",
        "    skip_connections = []\n",
        "\n",
        "    model = VGG19(include_top=False, weights='imagenet', input_tensor=inputs)\n",
        "    names = [\"block1_conv2\", \"block2_conv2\", \"block3_conv4\", \"block4_conv4\"]\n",
        "    for name in names:\n",
        "        skip_connections.append(model.get_layer(name).output)\n",
        "\n",
        "    output = model.get_layer(\"block5_conv4\").output\n",
        "    return output, skip_connections\n",
        "\n",
        "def decoder1(inputs, skip_connections):\n",
        "    num_filters = [256,128,64,32]\n",
        "    skip_connections.reverse()\n",
        "    x = inputs\n",
        "\n",
        "    for i, f in enumerate(num_filters):\n",
        "        x = UpSampling2D((2, 2), interpolation='bilinear')(x)\n",
        "        x = Concatenate()([x, skip_connections[i]])\n",
        "        x = conv_block(x, f)\n",
        "\n",
        "    return x\n",
        "\n",
        "def encoder2(inputs):\n",
        "    num_filters = [256,128,64,32]\n",
        "    skip_connections = []\n",
        "    x = inputs\n",
        "\n",
        "    for i, f in enumerate(num_filters):\n",
        "        x = conv_block(x, f)\n",
        "        skip_connections.append(x)\n",
        "        x = MaxPool2D((2, 2))(x)\n",
        "\n",
        "    return x, skip_connections\n",
        "\n",
        "def decoder2(inputs, skip_1, skip_2):\n",
        "    num_filters = [256,128,64,32]\n",
        "    skip_2.reverse()\n",
        "    x = inputs\n",
        "\n",
        "    for i, f in enumerate(num_filters):\n",
        "        x = UpSampling2D((2, 2), interpolation='bilinear')(x)\n",
        "        x = Concatenate()([x, skip_1[i], skip_2[i]])\n",
        "        x = conv_block(x, f)\n",
        "\n",
        "    return x\n",
        "\n",
        "def output_block(inputs):\n",
        "    x = Conv2D(1, (1, 1), padding=\"same\", activation='sigmoid')(inputs)\n",
        "    return x\n",
        "\n",
        "def Upsample(tensor, size):\n",
        "    \"\"\"Bilinear upsampling\"\"\"\n",
        "    def _upsample(x, size):\n",
        "        return tf.image.resize(images=x, size=size)\n",
        "    return Lambda(lambda x: _upsample(x, size), output_shape=size)(tensor)\n",
        "\n",
        "def ASPP(x, filter):\n",
        "    shape = x.shape\n",
        "\n",
        "    y1 = AveragePooling2D(pool_size=(shape[1], shape[2]))(x)\n",
        "    y1 = Conv2D(filter, 1, padding=\"same\")(y1)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "    y1 = Activation(\"relu\")(y1)\n",
        "    y1 = UpSampling2D((shape[1], shape[2]), interpolation='bilinear')(y1)\n",
        "\n",
        "    y2 = Conv2D(filter, 1, dilation_rate=1, padding=\"same\", use_bias=False)(x)\n",
        "    y2 = BatchNormalization()(y2)\n",
        "    y2 = Activation(\"relu\")(y2)\n",
        "\n",
        "    y3 = Conv2D(filter, 3, dilation_rate=6, padding=\"same\", use_bias=False)(x)\n",
        "    y3 = BatchNormalization()(y3)\n",
        "    y3 = Activation(\"relu\")(y3)\n",
        "\n",
        "    y4 = Conv2D(filter, 3, dilation_rate=12, padding=\"same\", use_bias=False)(x)\n",
        "    y4 = BatchNormalization()(y4)\n",
        "    y4 = Activation(\"relu\")(y4)\n",
        "\n",
        "    y5 = Conv2D(filter, 3, dilation_rate=18, padding=\"same\", use_bias=False)(x)\n",
        "    y5 = BatchNormalization()(y5)\n",
        "    y5 = Activation(\"relu\")(y5)\n",
        "\n",
        "    y = Concatenate()([y1, y2, y3, y4, y5])\n",
        "\n",
        "    y = Conv2D(filter, 1, dilation_rate=1, padding=\"same\", use_bias=False)(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation(\"relu\")(y)\n",
        "\n",
        "    return y\n",
        "\n",
        "def doubleunet(shape=(256,256,3)):\n",
        "    inputs = Input(shape)\n",
        "    x, skip_1 = encoder1(inputs)\n",
        "    x = ASPP(x, 64)\n",
        "    x = decoder1(x, skip_1)\n",
        "    outputs1 = output_block(x)\n",
        "\n",
        "    x = inputs * outputs1\n",
        "\n",
        "    x, skip_2 = encoder2(x)\n",
        "    x = ASPP(x, 64)\n",
        "    x = decoder2(x, skip_1, skip_2)\n",
        "    outputs2 = output_block(x)\n",
        "    outputs = Concatenate()([outputs1, outputs2])\n",
        "\n",
        "    model = Model(inputs, outputs2)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oP6k7Ka3P249"
      },
      "outputs": [],
      "source": [
        "seed_value = 58800\n",
        "loss = 'binary_crossentropy'\n",
        "ct = datetime.now()\n",
        "EPOCHS = 600\n",
        "dataset_type=\"cvcclinicdb\"\n",
        "model_type = \"doubleunet\"\n",
        "\n",
        "model_path = 'ModelSaveTensorFlow/' + dataset_type + '_' + model_type + 'newaug'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psMWxjF2P249"
      },
      "outputs": [],
      "source": [
        "from keras.optimizers import AdamW\n",
        "\n",
        "doubleunet_model = doubleunet(shape=(256,256,3))\n",
        "optimizer = AdamW(learning_rate = 0.00001, weight_decay = 0.004)\n",
        "loss = 'binary_crossentropy'\n",
        "doubleunet_model.compile(optimizer=optimizer, loss=loss,run_eagerly=True,\n",
        "                     metrics=['accuracy',f1_score,dice_coeff,dice_loss, total_loss, IoU, zero_IoU, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Q6WtevVzP249"
      },
      "outputs": [],
      "source": [
        "step = 0\n",
        "learning_rate = 1e-5\n",
        "min_loss_for_saving = 0.2\n",
        "batch_size=4\n",
        "epoch_step = len(X_train)//batch_size\n",
        "val_step = len(X_val)//batch_size\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(0, EPOCHS):\n",
        "\n",
        "    print(f'Training, epoch {epoch}')\n",
        "    print('Learning Rate: ' + str(learning_rate))\n",
        "\n",
        "    step += 1\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    image_augmented, mask_augmented = augment_images()\n",
        "\n",
        "    doubleunet_model.fit(x=image_augmented, y=mask_augmented, epochs=1,steps_per_epoch=epoch_step,validation_steps=val_step, validation_data=(X_val, y_val), verbose=1)\n",
        "\n",
        "    prediction_valid = doubleunet_model.predict(X_val, verbose=0, batch_size=4)\n",
        "    loss_valid = dice_metric_loss(y_val, prediction_valid)\n",
        "    loss_valid = loss_valid.numpy()\n",
        "    print(\"Loss Validation: \" + str(loss_valid))\n",
        "\n",
        "    prediction_test = doubleunet_model.predict(X_test, verbose=0, batch_size=4)\n",
        "    loss_test = dice_metric_loss(y_test, prediction_test)\n",
        "    loss_test = loss_test.numpy()\n",
        "    print(\"Loss Test: \" + str(loss_test))\n",
        "\n",
        "    if min_loss_for_saving > loss_valid:\n",
        "        min_loss_for_saving = loss_valid\n",
        "        print(\"Saved model with val_loss: \", loss_valid)\n",
        "        doubleunet_model.save(model_path)\n",
        "\n",
        "    del image_augmented\n",
        "    del mask_augmented\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.compat.v1.reset_default_graph()\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "hours, rem = divmod(elapsed_time, 3600)\n",
        "minutes, seconds = divmod(rem, 60)\n",
        "print(f\"Elapsed Time: {int(hours)}h {int(minutes)}m {int(seconds)}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMB1lCiyP24-"
      },
      "outputs": [],
      "source": [
        "del doubleunet_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Layer 1 Deeplabv3+ Model and Print the evaluation metrics"
      ],
      "metadata": {
        "id": "XMX9gtWtRKkb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTIP6OmEP24-"
      },
      "outputs": [],
      "source": [
        "print(\"Loading the model\")\n",
        "\n",
        "deeplabv3p_model = tf.keras.models.load_model(\"ModelSaveTensorFlow/cvcclinicdb_deeplabv3pnewaug\", custom_objects={'f1_score': f1_score,\n",
        "    'dice_coeff': dice_coeff,\n",
        "    'dice_loss': dice_loss,\n",
        "    'total_loss': total_loss,\n",
        "    'IoU': IoU,\n",
        "    'zero_IoU': zero_IoU,\n",
        "    'dice_metric_loss':dice_metric_loss})\n",
        "deeplabv3p_preds_tr = deeplabv3p_model.predict(X_train, batch_size=4)\n",
        "deeplabv3p_preds_val = deeplabv3p_model.predict(X_val, batch_size=4)\n",
        "deeplabv3p_preds_t = deeplabv3p_model.predict(X_test, batch_size=4)\n",
        "del deeplabv3p_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJX941ztP24-"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, jaccard_score\n",
        "\n",
        "ground_truth_flat = y_test.flatten()\n",
        "predictions_flat = np.round(deeplabv3p_preds_t).flatten()\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(ground_truth_flat, predictions_flat)\n",
        "precision = precision_score(ground_truth_flat, predictions_flat)\n",
        "recall = recall_score(ground_truth_flat, predictions_flat)\n",
        "f1 = f1_score(ground_truth_flat, predictions_flat)\n",
        "jaccard = jaccard_score(ground_truth_flat, predictions_flat)\n",
        "\n",
        "# Print or log the evaluation metrics\n",
        "print(\"Deeplab V3 Plus 1: \")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1}\")\n",
        "print(f\"IoU: {jaccard}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Layer 1 DoubleUNet Model and Print the evaluation metrics"
      ],
      "metadata": {
        "id": "U4t87ZGGRj3-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnPAzlfdP24-"
      },
      "outputs": [],
      "source": [
        "print(\"Loading the model\")\n",
        "\n",
        "doubleunet_model = tf.keras.models.load_model(\"ModelSaveTensorFlow/cvcclinicdb_doubleunetnewaug\", custom_objects={'f1_score': f1_score,\n",
        "    'dice_coeff': dice_coeff,\n",
        "    'dice_loss': dice_loss,\n",
        "    'total_loss': total_loss,\n",
        "    'IoU': IoU,\n",
        "    'zero_IoU': zero_IoU,\n",
        "    'dice_metric_loss':dice_metric_loss})\n",
        "doubleunet_preds_tr = doubleunet_model.predict(X_train, batch_size=4)\n",
        "doubleunet_preds_val = doubleunet_model.predict(X_val, batch_size=4)\n",
        "doubleunet_preds_t = doubleunet_model.predict(X_test, batch_size=4)\n",
        "del doubleunet_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgvWWB2gP24-"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, jaccard_score\n",
        "\n",
        "ground_truth_flat = y_test.flatten()\n",
        "predictions_flat = np.round(doubleunet_preds_t).flatten()\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(ground_truth_flat, predictions_flat)\n",
        "precision = precision_score(ground_truth_flat, predictions_flat)\n",
        "recall = recall_score(ground_truth_flat, predictions_flat)\n",
        "f1 = f1_score(ground_truth_flat, predictions_flat)\n",
        "jaccard = jaccard_score(ground_truth_flat, predictions_flat)\n",
        "\n",
        "# Print or log the evaluation metrics\n",
        "print(\"Double U-Net 1: \")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1}\")\n",
        "print(f\"IoU: {jaccard}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Average Layer 1"
      ],
      "metadata": {
        "id": "iXYCICCKRpWm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HjQTfyNP24-"
      },
      "outputs": [],
      "source": [
        "average_preds_tr = (doubleunet_preds_tr + deeplabv3p_preds_tr) / 2.0\n",
        "train_with_predictions = np.concatenate([X_train, average_preds_tr], axis=-1)\n",
        "\n",
        "average_preds_val = (doubleunet_preds_val + deeplabv3p_preds_val) / 2.0\n",
        "val_with_predictions = np.concatenate([X_val, average_preds_val], axis=-1)\n",
        "\n",
        "average_preds_t = (doubleunet_preds_t + deeplabv3p_preds_t) / 2.0\n",
        "test_with_predictions = np.concatenate([X_test, average_preds_t], axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layer 2 (U-Net and Deeplabv1)"
      ],
      "metadata": {
        "id": "79HDtTIgRua-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train U-Net Layer 2"
      ],
      "metadata": {
        "id": "YC47v6hiRvP0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nT-63LH_P24-"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "log_dir = './logs/fit' + datetime.now().strftime('%d.%m.%Y--%H-%M-%S')\n",
        "model_path = 'ModelSaveTensorFlow/cvcclinicdb_unet2_newaug.h5'\n",
        "checkpoint = ModelCheckpoint(model_path, monitor='val_dice_metric_loss', verbose = 1, save_best_only=True,\n",
        "                            mode='min', save_freq='epoch')\n",
        "early = EarlyStopping(monitor='val_dice_metric_loss', min_delta=0, patience = 25, verbose = 1, mode='min')\n",
        "board = TensorBoard(log_dir=log_dir,histogram_freq = 1)\n",
        "tensorboard_callback = [checkpoint,early,board]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "woHyfqtuP24_"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(learning_rate = 0.00001)\n",
        "loss = 'binary_crossentropy'\n",
        "unet_model2 = unet(input_shape=(256,256,4))\n",
        "unet_model2.compile(optimizer=optimizer, loss=loss, metrics=['accuracy',f1_score,dice_metric_loss,dice_coeff,dice_loss, total_loss, IoU, zero_IoU, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "unet_model2.fit(train_with_predictions, y_train,\n",
        "                  batch_size=8,epochs=400,\n",
        "                  validation_data=(val_with_predictions, y_val),\n",
        "                  callbacks=[tensorboard_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Deeplabv1 Layer 2"
      ],
      "metadata": {
        "id": "OsGiGyz9Ry-M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62jaawdJP24_"
      },
      "outputs": [],
      "source": [
        "def deeplab(input_shape=(256,256,3)):\n",
        "\n",
        "    atrous_rates = [6, 12, 18, 24]\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
        "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
        "\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
        "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
        "\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
        "    pool3 = MaxPooling2D((2, 2))(conv3)\n",
        "\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "\n",
        "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
        "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
        "\n",
        "    fusion_layer_deeplab = pool3\n",
        "\n",
        "    # Atrous (dilated) Convolutions with Different Rates\n",
        "    atrous_layers = []\n",
        "    for rate in atrous_rates:\n",
        "        atrous_layer = Conv2D(512, 3, activation='relu', padding='same', dilation_rate=rate)(conv5)\n",
        "        atrous_layers.append(atrous_layer)\n",
        "\n",
        "    # Concatenated Atrous Layers\n",
        "    concat = Concatenate(axis=-1)(atrous_layers)\n",
        "\n",
        "    # Decoder\n",
        "    conv6 = Conv2D(512, 1, activation='relu', padding='same')(concat)\n",
        "\n",
        "    upsample = Conv2DTranspose(1, kernel_size=16, strides=16, padding='same')(conv6)\n",
        "    outputs = Conv2D(1, 1, activation='sigmoid')(upsample)\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29TauihqP24_"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "log_dir = './logs/fit' + datetime.now().strftime('%d.%m.%Y--%H-%M-%S')\n",
        "model_path = 'ModelSaveTensorFlow/cvcclinicdb_deeplab2_newaug.h5'\n",
        "checkpoint = ModelCheckpoint(model_path, monitor='val_dice_metric_loss', verbose = 1, save_best_only=True,\n",
        "                            mode='min', save_freq='epoch')\n",
        "early = EarlyStopping(monitor='val_dice_metric_loss', min_delta=0, patience = 25, verbose = 1, mode='min')\n",
        "board = TensorBoard(log_dir=log_dir,histogram_freq = 1)\n",
        "tensorboard_callback = [checkpoint,early,board]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "bgGWRFobP25A"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(learning_rate = 0.00001,weight_decay=0.004)\n",
        "loss = 'binary_crossentropy'\n",
        "deeplab_model2 = deeplab(input_shape=(256,256,4))\n",
        "deeplab_model2.compile(optimizer=optimizer, loss=loss, metrics=['accuracy',f1_score,dice_metric_loss,dice_coeff,dice_loss, total_loss, IoU, zero_IoU, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "deeplab_model2.fit(train_with_predictions, y_train,\n",
        "                  batch_size=8,epochs=400,\n",
        "                  validation_data=(val_with_predictions, y_val),\n",
        "                  callbacks=[tensorboard_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load U-Net Layer 2 and print the evaluation metrics"
      ],
      "metadata": {
        "id": "vQvzF44sSAiF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGe6KEosP24_"
      },
      "outputs": [],
      "source": [
        "unet2_model = tf.keras.models.load_model('ModelSaveTensorFlow/cvcclinicdb_unet2_newaug.h5', custom_objects={'f1_score': f1_score,\n",
        "        'dice_coeff': dice_coeff,\n",
        "        'dice_loss': dice_loss,\n",
        "        'total_loss': total_loss,\n",
        "        'IoU': IoU,\n",
        "        'zero_IoU': zero_IoU,\n",
        "        'dice_metric_loss':dice_metric_loss})\n",
        "unet2_preds_t = unet2_model.predict(test_with_predictions,batch_size=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYYMmwbpP24_"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, jaccard_score\n",
        "\n",
        "ground_truth_flat = y_test.flatten()\n",
        "predictions_flat = np.round(unet2_preds_t).flatten()\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(ground_truth_flat, predictions_flat)\n",
        "precision = precision_score(ground_truth_flat, predictions_flat)\n",
        "recall = recall_score(ground_truth_flat, predictions_flat)\n",
        "f1 = f1_score(ground_truth_flat, predictions_flat)\n",
        "jaccard = jaccard_score(ground_truth_flat, predictions_flat)\n",
        "\n",
        "# Print or log the evaluation metrics\n",
        "print(\"U-Net 2: \")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1}\")\n",
        "print(f\"IoU: {jaccard}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Deeplabv1 Layer 2 and print the evaluation metrics"
      ],
      "metadata": {
        "id": "3IdUn5lRS-dC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyUjCEyiP25A"
      },
      "outputs": [],
      "source": [
        "deeplab2_model = tf.keras.models.load_model(\"ModelSaveTensorFlow/cvcclinicdb_deeplab2_newaug.h5\", custom_objects={'f1_score': f1_score,\n",
        "    'dice_coeff': dice_coeff,\n",
        "    'dice_loss': dice_loss,\n",
        "    'total_loss': total_loss,\n",
        "    'IoU': IoU,\n",
        "    'zero_IoU': zero_IoU,\n",
        "    'dice_metric_loss':dice_metric_loss})\n",
        "deeplab2_preds_t = deeplab2_model.predict(test_with_predictions,batch_size=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dr0GPS9UP25F"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, jaccard_score\n",
        "\n",
        "ground_truth_flat = y_test.flatten()\n",
        "predictions_flat = np.round(deeplab2_preds_t).flatten()\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(ground_truth_flat, predictions_flat)\n",
        "precision = precision_score(ground_truth_flat, predictions_flat)\n",
        "recall = recall_score(ground_truth_flat, predictions_flat)\n",
        "f1 = f1_score(ground_truth_flat, predictions_flat)\n",
        "jaccard = jaccard_score(ground_truth_flat, predictions_flat)\n",
        "\n",
        "# Print or log the evaluation metrics\n",
        "print(\"Deeplab 2: \")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1}\")\n",
        "print(f\"IoU: {jaccard}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsuG1-mzP25F"
      },
      "outputs": [],
      "source": [
        "average2_preds_test = (unet2_preds_t + deeplab2_preds_t) / 2.0\n",
        "average2_preds_t = np.round(average2_preds_test).flatten()\n",
        "y_test_flat = y_test.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cvOz4qzP25F"
      },
      "outputs": [],
      "source": [
        "avg_ensemble_accuracy = accuracy_score(y_test_flat, average2_preds_t)\n",
        "avg_ensemble_f1 = f1_score(y_test_flat, average2_preds_t)\n",
        "avg_ensemble_precision = precision_score(y_test_flat, average2_preds_t)\n",
        "avg_ensemble_recall = recall_score(y_test_flat, average2_preds_t)\n",
        "avg_ensemble_iou = jaccard_score(y_test_flat, average2_preds_t)\n",
        "\n",
        "# Display ensemble metrics\n",
        "print(\"Averaged Ensemble Accuracy:\", avg_ensemble_accuracy)\n",
        "print(\"Averaged Ensemble F1 Score:\", avg_ensemble_f1)\n",
        "print(\"Averaged Ensemble Precision:\", avg_ensemble_precision)\n",
        "print(\"Averaged Ensemble Recall:\", avg_ensemble_recall)\n",
        "print(\"Averaged Ensemble IoU:\", avg_ensemble_iou)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization of Models from layer 1, layer 2, and final output"
      ],
      "metadata": {
        "id": "19XdHhfITC0w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Pyb4msDbZMR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "num_samples_to_visualize = 20\n",
        "random_indices = np.random.choice(range(len(X_test)), num_samples_to_visualize, replace=False)\n",
        "\n",
        "# Visualize each selected sample\n",
        "for sample_index in random_indices:\n",
        "    # Load the original image and corresponding mask\n",
        "    original_image = X_test[sample_index]\n",
        "    actual_mask = y_test[sample_index]\n",
        "\n",
        "    # Get predictions for the chosen sample\n",
        "    doubleunet_prediction = np.round(doubleunet_preds_t[sample_index, ..., 0])\n",
        "    deeplabv3p_prediction = np.round(deeplabv3p_preds_t[sample_index, ..., 0])\n",
        "    avg_prediction = np.round(average_preds_test[sample_index])\n",
        "    unet2_prediction = np.round(unet2_preds_t[sample_index, ..., 0])\n",
        "    deeplab2_prediction = np.round(deeplab2_preds_t[sample_index, ..., 0])\n",
        "    avg2_prediction = np.round(average2_preds_test[sample_index])\n",
        "    # Plot the images and predictions\n",
        "    plt.figure(figsize=(20, 20))\n",
        "\n",
        "    plt.subplot(1, 11, 1)\n",
        "    plt.imshow(original_image)\n",
        "    plt.title(\"Original Image\")\n",
        "\n",
        "    plt.subplot(1, 11, 2)\n",
        "    plt.imshow(original_image[:, :, 0], cmap='Reds')\n",
        "    plt.title(\"Red Channel\")\n",
        "\n",
        "    plt.subplot(1, 11, 3)\n",
        "    plt.imshow(original_image[:, :, 1], cmap='Greens')\n",
        "    plt.title(\"Green Channel\")\n",
        "\n",
        "    plt.subplot(1, 11, 4)\n",
        "    plt.imshow(original_image[:, :, 2], cmap='Blues')\n",
        "    plt.title(\"Blue Channel\")\n",
        "\n",
        "    plt.subplot(1, 11, 5)\n",
        "    plt.imshow(actual_mask, cmap='gray')\n",
        "    plt.title(\"Actual Mask\")\n",
        "\n",
        "    plt.subplot(1, 11, 6)\n",
        "    plt.imshow(doubleunet_prediction, cmap='gray')\n",
        "    plt.title(\"DoubleU-Net\")\n",
        "\n",
        "    plt.subplot(1, 11, 7)\n",
        "    plt.imshow(deeplabv3p_prediction, cmap='gray')\n",
        "    plt.title(\"Deeplab V3 Plus\")\n",
        "\n",
        "    plt.subplot(1, 11, 8)\n",
        "    plt.imshow(avg_prediction, cmap='gray')\n",
        "    plt.title(\"Avg Layer 1\")\n",
        "\n",
        "    plt.subplot(1, 11, 9)\n",
        "    plt.imshow(unet2_prediction, cmap='gray')\n",
        "    plt.title(\"U-Net 2\")\n",
        "\n",
        "    plt.subplot(1, 11, 10)\n",
        "    plt.imshow(deeplab2_prediction, cmap='gray')\n",
        "    plt.title(\"Deeplab 2\")\n",
        "\n",
        "    plt.subplot(1, 11, 11)\n",
        "    plt.imshow(avg2_prediction, cmap='gray')\n",
        "    plt.title(\"Avg Layer 2\")\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "EZc3w2cTQp2n"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}