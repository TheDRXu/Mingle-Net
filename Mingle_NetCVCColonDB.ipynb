{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_tlZ3i7w3ud",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import glob\n",
        "import cv2\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import pickle\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Reshape, concatenate, Concatenate, Conv2D, Conv2DTranspose, MaxPooling2D, UpSampling2D\n",
        "from tensorflow.keras.layers import Input, Activation, BatchNormalization, Dropout, Lambda, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate\n",
        "from tensorflow.keras.models import Model, load_model, save_model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping,TensorBoard,LearningRateScheduler\n",
        "\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from skimage.color import rgb2gray as rtg\n",
        "from skimage.io import imread, imshow\n",
        "from skimage.transform import resize\n",
        "from PIL import Image\n",
        "from datetime import datetime\n",
        "import time\n",
        "from keras.callbacks import CSVLogger\n",
        "from keras.optimizers import AdamW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbNQZpqOoyJY"
      },
      "source": [
        "# PreProcessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sawusfw8yd6l"
      },
      "outputs": [],
      "source": [
        "def load_data(img_height, img_width, images_to_be_loaded):\n",
        "    IMAGES_PATH = \"CVC-ColonDB/images/\"\n",
        "    MASKS_PATH = \"CVC-ColonDB/masks/\"\n",
        "\n",
        "    train_ids = glob(IMAGES_PATH + \"*.png\")\n",
        "\n",
        "    train_ids = sorted(train_ids)\n",
        "\n",
        "    if images_to_be_loaded == -1:\n",
        "        images_to_be_loaded = len(train_ids)\n",
        "\n",
        "    #Create Empty arrays to store data image.\n",
        "    X_train = np.zeros((images_to_be_loaded, img_height, img_width, 3), dtype=np.float32)\n",
        "    Y_train = np.zeros((images_to_be_loaded, img_height, img_width), dtype=np.uint8)\n",
        "\n",
        "    print('Resizing training images and masks: ' + str(images_to_be_loaded))\n",
        "    #Iterate over and load the corresponding image and mask\n",
        "    for n, id_ in tqdm(enumerate(train_ids)):\n",
        "        if n == images_to_be_loaded:\n",
        "            break\n",
        "\n",
        "        image_path = id_\n",
        "        mask_path = image_path.replace(\"images\", \"masks\")\n",
        "\n",
        "        image = imread(image_path)\n",
        "        mask_ = imread(mask_path)\n",
        "\n",
        "        mask = np.zeros((img_height, img_width), dtype=np.bool_)\n",
        "\n",
        "        pillow_image = Image.fromarray(image)\n",
        "\n",
        "        #Resize the image\n",
        "        pillow_image = pillow_image.resize((img_height, img_width))\n",
        "        image = np.array(pillow_image)\n",
        "\n",
        "        #Normalize the pixel values\n",
        "        X_train[n] = image / 255\n",
        "\n",
        "        pillow_mask = Image.fromarray(mask_)\n",
        "        pillow_mask = pillow_mask.resize((img_height, img_width), resample=Image.LANCZOS)\n",
        "        mask_ = np.array(pillow_mask)\n",
        "\n",
        "        #Convert mask into binary array based on the threshold\n",
        "        for i in range(img_height):\n",
        "            for j in range(img_width):\n",
        "                if np.any(mask_[i, j] >= 127):\n",
        "                    mask[i, j] = 1\n",
        "        #Store mask data\n",
        "        Y_train[n] = mask\n",
        "    #Expand dimension of mask array\n",
        "    Y_train = np.expand_dims(Y_train, axis=-1)\n",
        "\n",
        "    return X_train, Y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTx39qbgVVit"
      },
      "outputs": [],
      "source": [
        "img_size = 352\n",
        "X, Y = load_data(img_size, img_size, -1) #Resize (352,352) for Kvasir-SEG & CVC-ColonDB while (256,256) for CVC-ClinicDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sosaSqupU6qQ"
      },
      "outputs": [],
      "source": [
        "test_size = 0.2\n",
        "validation_size = 0.5\n",
        "\n",
        "#Split the the data into train:validation:test with 8:1:1 Ratio\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=0)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=validation_size, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7W_bCwUVViu"
      },
      "source": [
        "## Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAlJf6snzg_5"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "from albumentations.augmentations import functional as F\n",
        "\n",
        "#Augmentations Steps\n",
        "aug_train = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.ColorJitter(brightness=(0.6,1.6), contrast=0.2, saturation=0.1, hue=0.01, always_apply=True),\n",
        "    A.Affine(scale=(0.5,1.5), translate_percent=(-0.125,0.125), rotate=(-180,180), shear=(-22.5,22), always_apply=True),\n",
        "    A.GaussianBlur(blur_limit=(25, 25),sigma_limit=(0.001, 2.0),always_apply=False, p=1.0),\n",
        "])\n",
        "\n",
        "#Augmentation function for training\n",
        "def augment_images():\n",
        "    x_train_out = []\n",
        "    y_train_out = []\n",
        "\n",
        "    for i in range (len(X_train)):\n",
        "        ug = aug_train(image=X_train[i], mask=y_train[i])\n",
        "        x_train_out.append(ug['image'])\n",
        "        y_train_out.append(ug['mask'])\n",
        "\n",
        "    return np.array(x_train_out), np.array(y_train_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Metrics"
      ],
      "metadata": {
        "id": "UbCZKSUqKCE1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "23SX_iGf3B6p"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "import numpy as np\n",
        "def dice_coeff(y_true, y_pred):\n",
        "\n",
        "    _epsilon = 10 ** -7\n",
        "    intersections = tf.reduce_sum(y_true * y_pred)\n",
        "    unions = tf.reduce_sum(y_true + y_pred)\n",
        "    dice_scores = (2.0 * intersections + _epsilon) / (unions + _epsilon)\n",
        "\n",
        "    return dice_scores\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "\n",
        "    loss = 1 - dice_coeff(y_true, y_pred)\n",
        "\n",
        "    return loss\n",
        "\n",
        "def total_loss(y_true, y_pred):\n",
        "    return 0.5*binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "\n",
        "def IoU(y_true, y_pred, eps=1e-6):\n",
        "\n",
        "    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
        "    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3]) - intersection\n",
        "\n",
        "    return K.mean( (intersection + eps) / (union + eps), axis=0)\n",
        "\n",
        "def zero_IoU(y_true, y_pred):\n",
        "\n",
        "    return IoU(1-y_true, 1-y_pred)\n",
        "\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "\n",
        "    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "\n",
        "def tversky(y_true, y_pred, smooth=1, alpha=0.7):\n",
        "\n",
        "    y_true_pos = tf.reshape(y_true,[-1])\n",
        "    y_pred_pos = tf.reshape(y_pred,[-1])\n",
        "    true_pos = tf.reduce_sum(y_true_pos * y_pred_pos)\n",
        "    false_neg = tf.reduce_sum(y_true_pos * (1 - y_pred_pos))\n",
        "    false_pos = tf.reduce_sum((1 - y_true_pos) * y_pred_pos)\n",
        "\n",
        "    return (true_pos + smooth) / (true_pos + alpha * false_neg + (1 - alpha) * false_pos + smooth)\n",
        "\n",
        "\n",
        "def tversky_loss(y_true, y_pred):\n",
        "\n",
        "    return 1 - tversky(y_true, y_pred)\n",
        "\n",
        "def focal_tversky_loss(y_true, y_pred, gamma=0.75):\n",
        "\n",
        "    tv = tversky(y_true, y_pred)\n",
        "\n",
        "    return K.pow((1 - tv), gamma)\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(tf.math.round(y_pred), tf.float32)\n",
        "\n",
        "    tp = tf.reduce_sum(y_true * y_pred)\n",
        "    fp = tf.reduce_sum((1 - y_true) * y_pred)\n",
        "    fn = tf.reduce_sum(y_true * (1 - y_pred))\n",
        "\n",
        "    precision = tp / (tp + fp + tf.keras.backend.epsilon())\n",
        "    recall = tp / (tp + fn + tf.keras.backend.epsilon())\n",
        "\n",
        "    f1 = 2 * precision * recall / (precision + recall + tf.keras.backend.epsilon())\n",
        "    return f1\n",
        "\n",
        "def dice_metric_loss(ground_truth, predictions, smooth=1e-6):\n",
        "    ground_truth = K.cast(ground_truth, tf.float32)\n",
        "    predictions = K.cast(predictions, tf.float32)\n",
        "    ground_truth = K.flatten(ground_truth)\n",
        "    predictions = K.flatten(predictions)\n",
        "    intersection = K.sum(predictions * ground_truth)\n",
        "    union = K.sum(predictions) + K.sum(ground_truth)\n",
        "\n",
        "    dice = (2. * intersection + smooth) / (union + smooth)\n",
        "\n",
        "    return 1 - dice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efSwj3xnVViv"
      },
      "source": [
        "# Layer 1 (Deeplabv3+ and DoubleUNet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMilOGujVViv"
      },
      "source": [
        "## Train Deeplabv3+ Layer 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aS9XD74xXIQa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
        "\n",
        "from keras.layers import (\n",
        "    Conv2D,\n",
        "    BatchNormalization,\n",
        "    Activation,\n",
        "    Concatenate,\n",
        "    Input,\n",
        "    Dropout,\n",
        ")\n",
        "from keras.layers import (\n",
        "    AveragePooling2D,\n",
        "    GlobalAveragePooling2D,\n",
        "    UpSampling2D,\n",
        "    Reshape,\n",
        "    Dense,\n",
        ")\n",
        "from keras.models import Model\n",
        "from keras.applications import ResNet50, ResNet101\n",
        "from keras.regularizers import l2\n",
        "from tensorflow.python.keras.engine.keras_tensor import KerasTensor\n",
        "\n",
        "\n",
        "def squeeze_and_excite(inputs: KerasTensor, ratio: int = 8) -> KerasTensor:\n",
        "    init = inputs\n",
        "    filters = init.shape[-1]\n",
        "    se_shape = (1, 1, filters)\n",
        "\n",
        "    se = GlobalAveragePooling2D()(init)\n",
        "    se = Reshape(se_shape)(se)\n",
        "    se = Dense(\n",
        "        filters // ratio,\n",
        "        activation=\"relu\",\n",
        "        kernel_initializer=\"he_normal\",\n",
        "        use_bias=False,\n",
        "    )(se)\n",
        "    se = Dense(\n",
        "        filters,\n",
        "        activation=\"sigmoid\",\n",
        "        kernel_initializer=\"he_normal\",\n",
        "        use_bias=False,\n",
        "    )(se)\n",
        "\n",
        "    x = init * se\n",
        "    return x\n",
        "\n",
        "\n",
        "def ASPP(inputs: KerasTensor) -> KerasTensor:\n",
        "    shape = inputs.shape\n",
        "    y1 = AveragePooling2D(pool_size=(shape[1], shape[2]))(inputs)\n",
        "    y1 = Conv2D(256, 1, padding=\"same\", use_bias=False, kernel_initializer=\"he_normal\")(y1)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "    y1 = Activation(\"relu\")(y1)\n",
        "    y1 = UpSampling2D((shape[1], shape[2]), interpolation=\"bilinear\")(y1)\n",
        "\n",
        "    # 1x1 Convolution\n",
        "    y2 = Conv2D(256, 1, padding=\"same\", use_bias=False, kernel_initializer=\"he_normal\")(inputs)\n",
        "    y2 = BatchNormalization()(y2)\n",
        "    # y2 = Dropout(0.5)(y2)\n",
        "    y2 = Activation(\"relu\")(y2)\n",
        "\n",
        "    # 3x3 Convolution, Dilation Rate - 12 or 6\n",
        "    y3 = Conv2D(\n",
        "        256, 3, padding=\"same\", dilation_rate=6, use_bias=False, kernel_initializer=\"he_normal\"\n",
        "    )(inputs)\n",
        "    y3 = BatchNormalization()(y3)\n",
        "    y3 = Activation(\"relu\")(y3)\n",
        "\n",
        "    # 3x3 Convolution, Dilation Rate - 24 or 12\n",
        "    y4 = Conv2D(\n",
        "        256, 3, padding=\"same\", dilation_rate=12, use_bias=False, kernel_initializer=\"he_normal\"\n",
        "    )(inputs)\n",
        "    y4 = BatchNormalization()(y4)\n",
        "    y4 = Activation(\"relu\")(y4)\n",
        "\n",
        "    # 3x3 Convolution, Dilation Rate - 36 or 18\n",
        "    y5 = Conv2D(\n",
        "        256, 3, padding=\"same\", dilation_rate=18, use_bias=False, kernel_initializer=\"he_normal\"\n",
        "    )(inputs)\n",
        "    y5 = BatchNormalization()(y5)\n",
        "    y5 = Activation(\"relu\")(y5)\n",
        "\n",
        "    # 1x1 Convolution on the concatenated Feature Map\n",
        "    y = Concatenate()([y1, y2, y3, y4, y5])\n",
        "    y = Conv2D(256, 1, padding=\"same\", use_bias=False, kernel_initializer=\"he_normal\")(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation(\"relu\")(y)\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "def deeplabModel(modelType: str, shape: tuple[int] = (352, 352, 3)) -> Model:\n",
        "    inputs = Input(shape)  # instantiating a tensor\n",
        "\n",
        "    encoder = (\n",
        "        ResNet101(weights=\"imagenet\", include_top=False, input_tensor=inputs)\n",
        "        if modelType == \"ResNet101\"\n",
        "        else ResNet50(weights=\"imagenet\", include_top=False, input_tensor=inputs)\n",
        "    )\n",
        "\n",
        "    image_features = encoder.get_layer(\n",
        "        \"conv4_block23_out\" if modelType == \"ResNet101\" else \"conv4_block6_out\"\n",
        "    ).output\n",
        "\n",
        "    # High-Level Features\n",
        "    x_a = ASPP(image_features)\n",
        "    # Up-Sampling High-Level Features by 4\n",
        "    x_a = UpSampling2D((4, 4), interpolation=\"bilinear\")(x_a)\n",
        "    x_a = Dropout(0.5)(x_a)\n",
        "\n",
        "    # Low-Level Features\n",
        "    x_b = encoder.get_layer(\"conv2_block2_out\").output\n",
        "\n",
        "    # 1x1 Convolution on Low-Level Features\n",
        "    x_b = Conv2D(\n",
        "        filters=48, kernel_size=1, padding=\"same\", use_bias=False, kernel_initializer=\"he_normal\"\n",
        "    )(x_b)\n",
        "    x_b = BatchNormalization()(x_b)\n",
        "    x_b = Activation(\"relu\")(x_b)\n",
        "\n",
        "    # Concatenating High-Level and Low-Level Features\n",
        "    x = Concatenate()([x_a, x_b])\n",
        "    x = Dropout(0.5)(x)\n",
        "    # x = squeeze_and_excite(x)\n",
        "\n",
        "    # 3x3 Convolution on Concatenated Map\n",
        "    x = Conv2D(\n",
        "        filters=256, kernel_size=3, padding=\"same\", use_bias=False, kernel_initializer=\"he_normal\"\n",
        "    )(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = squeeze_and_excite(x)\n",
        "\n",
        "    # 3x3 Convolution on Concatenated Map\n",
        "    x = Conv2D(\n",
        "        filters=256, kernel_size=3, padding=\"same\", use_bias=False, kernel_initializer=\"he_normal\"\n",
        "    )(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    # x = Dropout(0.5)(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = squeeze_and_excite(x)\n",
        "\n",
        "    # Final Up-Sampling by 4\n",
        "    x = UpSampling2D((4, 4), interpolation=\"bilinear\")(x)\n",
        "    x = Conv2D(1, 1)(x)\n",
        "    x = Activation(\"sigmoid\")(x)\n",
        "\n",
        "    model = Model(inputs, x)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "g3wjYeM8VViv"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 600\n",
        "dataset_type=\"kvasir\"\n",
        "model_type = \"deeplabv3p\"\n",
        "\n",
        "model_path = '/home/kchang/Downloads/MingleNet/ModelSaveTensorFlow/' + dataset_type + '_' + model_type + 'newaug'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFob2psXVViv"
      },
      "outputs": [],
      "source": [
        "#Compile DeepLabV3 Model\n",
        "deeplabv3p_model = deeplabModel(modelType=\"ResNet101\",shape=(352,352,3))\n",
        "optimizer = AdamW(learning_rate = 0.00001, weight_decay = 0.004)\n",
        "loss = 'binary_crossentropy'\n",
        "deeplabv3p_model.compile(optimizer=optimizer, loss=loss,\n",
        "                     metrics=['accuracy',dice_coeff, IoU, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtZW5J1jVViv",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-5\n",
        "min_loss_for_saving = 0.3\n",
        "start_time = time.time()\n",
        "\n",
        "print(\"Learning rate \",learning_rate)\n",
        "#Loop Through Epochs\n",
        "for epoch in range(0, EPOCHS):\n",
        "\n",
        "    print(f'Training, epoch {epoch+1}')\n",
        "\n",
        "    step += 1\n",
        "    #Clear Keras Session\n",
        "    tf.keras.backend.clear_session()\n",
        "    #Create new augmented image instances\n",
        "    image_augmented, mask_augmented = augment_images()\n",
        "    #Train Model\n",
        "    deeplabv3p_model.fit(x=image_augmented, y=mask_augmented, epochs=1, batch_size=8, validation_data=(X_val, y_val), verbose=1)\n",
        "\n",
        "    #Evaluate Loss Validation\n",
        "    prediction_valid = deeplabv3p_model.predict(X_val, verbose=0, batch_size=8)\n",
        "    loss_valid = dice_metric_loss(y_val, prediction_valid)\n",
        "    loss_valid = loss_valid.numpy()\n",
        "    print(\"Loss Validation: \" + str(loss_valid))\n",
        "\n",
        "    #Evaluate Loss Test\n",
        "    prediction_test = deeplabv3p_model.predict(X_test, verbose=0, batch_size=8)\n",
        "    loss_test = dice_metric_loss(y_test, prediction_test)\n",
        "    loss_test = loss_test.numpy()\n",
        "    print(\"Loss Test: \" + str(loss_test))\n",
        "\n",
        "    #min_loss = 0.2 if the mode validation loss surpassed the minimum save the model\n",
        "    if min_loss_for_saving > loss_valid:\n",
        "        min_loss_for_saving = loss_valid\n",
        "        print(\"Saved model with val_loss: \", loss_valid)\n",
        "        deeplabv3p_model.save(model_path)\n",
        "\n",
        "    #Delete the augmented image instances\n",
        "    del image_augmented\n",
        "    del mask_augmented\n",
        "    tf.compat.v1.reset_default_graph()\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "hours, rem = divmod(elapsed_time, 3600)\n",
        "minutes, seconds = divmod(rem, 60)\n",
        "print(f\"Elapsed Time: {int(hours)}h {int(minutes)}m {int(seconds)}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyj1jkLlVViw"
      },
      "outputs": [],
      "source": [
        "del deeplabv3p_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxwGR1eMVViw"
      },
      "source": [
        "## Train DoubleUNet Layer 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHEPnG8lX9VG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import *\n",
        "\n",
        "def squeeze_excite_block(inputs, ratio=8):\n",
        "    init = inputs\n",
        "    channel_axis = -1\n",
        "    filters = init.shape[channel_axis]\n",
        "    se_shape = (1, 1, filters)\n",
        "\n",
        "    se = GlobalAveragePooling2D()(init)\n",
        "    se = Reshape(se_shape)(se)\n",
        "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "\n",
        "    x = Multiply()([init, se])\n",
        "    return x\n",
        "\n",
        "def conv_block(inputs, filters):\n",
        "    x = inputs\n",
        "\n",
        "    x = Conv2D(filters, (3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters, (3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = squeeze_excite_block(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def encoder1(inputs):\n",
        "    skip_connections = []\n",
        "\n",
        "    model = VGG19(include_top=False, weights='imagenet', input_tensor=inputs)\n",
        "    names = [\"block1_conv2\", \"block2_conv2\", \"block3_conv4\", \"block4_conv4\"]\n",
        "    for name in names:\n",
        "        skip_connections.append(model.get_layer(name).output)\n",
        "\n",
        "    output = model.get_layer(\"block5_conv4\").output\n",
        "    return output, skip_connections\n",
        "\n",
        "def decoder1(inputs, skip_connections):\n",
        "    num_filters = [352, 176, 88, 44]\n",
        "    skip_connections.reverse()\n",
        "    x = inputs\n",
        "\n",
        "    for i, f in enumerate(num_filters):\n",
        "        x = UpSampling2D((2, 2), interpolation='bilinear')(x)\n",
        "        x = Concatenate()([x, skip_connections[i]])\n",
        "        x = conv_block(x, f)\n",
        "\n",
        "    return x\n",
        "\n",
        "def encoder2(inputs):\n",
        "    num_filters = [352, 176, 88, 44]\n",
        "    skip_connections = []\n",
        "    x = inputs\n",
        "\n",
        "    for i, f in enumerate(num_filters):\n",
        "        x = conv_block(x, f)\n",
        "        skip_connections.append(x)\n",
        "        x = MaxPool2D((2, 2))(x)\n",
        "\n",
        "    return x, skip_connections\n",
        "\n",
        "def decoder2(inputs, skip_1, skip_2):\n",
        "    num_filters = [352, 176, 88, 44]\n",
        "    skip_2.reverse()\n",
        "    x = inputs\n",
        "\n",
        "    for i, f in enumerate(num_filters):\n",
        "        x = UpSampling2D((2, 2), interpolation='bilinear')(x)\n",
        "        x = Concatenate()([x, skip_1[i], skip_2[i]])\n",
        "        x = conv_block(x, f)\n",
        "\n",
        "    return x\n",
        "\n",
        "def output_block(inputs):\n",
        "    x = Conv2D(1, (1, 1), padding=\"same\", activation='sigmoid')(inputs)\n",
        "    return x\n",
        "\n",
        "def Upsample(tensor, size):\n",
        "    \"\"\"Bilinear upsampling\"\"\"\n",
        "    def _upsample(x, size):\n",
        "        return tf.image.resize(images=x, size=size)\n",
        "    return Lambda(lambda x: _upsample(x, size), output_shape=size)(tensor)\n",
        "\n",
        "def ASPP(x, filter):\n",
        "    shape = x.shape\n",
        "\n",
        "    y1 = AveragePooling2D(pool_size=(shape[1], shape[2]))(x)\n",
        "    y1 = Conv2D(filter, 1, padding=\"same\")(y1)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "    y1 = Activation(\"relu\")(y1)\n",
        "    y1 = UpSampling2D((shape[1], shape[2]), interpolation='bilinear')(y1)\n",
        "\n",
        "    y2 = Conv2D(filter, 1, dilation_rate=1, padding=\"same\", use_bias=False)(x)\n",
        "    y2 = BatchNormalization()(y2)\n",
        "    y2 = Activation(\"relu\")(y2)\n",
        "\n",
        "    y3 = Conv2D(filter, 3, dilation_rate=6, padding=\"same\", use_bias=False)(x)\n",
        "    y3 = BatchNormalization()(y3)\n",
        "    y3 = Activation(\"relu\")(y3)\n",
        "\n",
        "    y4 = Conv2D(filter, 3, dilation_rate=12, padding=\"same\", use_bias=False)(x)\n",
        "    y4 = BatchNormalization()(y4)\n",
        "    y4 = Activation(\"relu\")(y4)\n",
        "\n",
        "    y5 = Conv2D(filter, 3, dilation_rate=18, padding=\"same\", use_bias=False)(x)\n",
        "    y5 = BatchNormalization()(y5)\n",
        "    y5 = Activation(\"relu\")(y5)\n",
        "\n",
        "    y = Concatenate()([y1, y2, y3, y4, y5])\n",
        "\n",
        "    y = Conv2D(filter, 1, dilation_rate=1, padding=\"same\", use_bias=False)(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation(\"relu\")(y)\n",
        "\n",
        "    return y\n",
        "\n",
        "def double_unet(shape=(352,352,3)):\n",
        "    inputs = Input(shape)\n",
        "    x, skip_1 = encoder1(inputs)\n",
        "    x = ASPP(x, 64)\n",
        "    x = decoder1(x, skip_1)\n",
        "    outputs1 = output_block(x)\n",
        "\n",
        "    x = inputs * outputs1\n",
        "\n",
        "    x, skip_2 = encoder2(x)\n",
        "    x = ASPP(x, 64)\n",
        "    x = decoder2(x, skip_1, skip_2)\n",
        "    outputs2 = output_block(x)\n",
        "    outputs = Concatenate()([outputs1, outputs2])\n",
        "\n",
        "    model = Model(inputs, outputs2)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKqQUhq0VViw"
      },
      "outputs": [],
      "source": [
        "def f1_score(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(tf.math.round(y_pred), tf.float32)\n",
        "\n",
        "    tp = tf.reduce_sum(y_true * y_pred)\n",
        "    fp = tf.reduce_sum((1 - y_true) * y_pred)\n",
        "    fn = tf.reduce_sum(y_true * (1 - y_pred))\n",
        "\n",
        "    precision = tp / (tp + fp + tf.keras.backend.epsilon())\n",
        "    recall = tp / (tp + fn + tf.keras.backend.epsilon())\n",
        "\n",
        "    f1 = 2 * precision * recall / (precision + recall + tf.keras.backend.epsilon())\n",
        "    return f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-pFV8cIVViw"
      },
      "outputs": [],
      "source": [
        "loss = 'binary_crossentropy'\n",
        "ct = datetime.now()\n",
        "EPOCHS = 600\n",
        "dataset_type=\"kvasir\"\n",
        "model_type = \"doubleunet\"\n",
        "\n",
        "progress_path = '/home/kchang/Downloads/MingleNet/ProgressFull/' + dataset_type + '_progress_csv_' + model_type  +  '_' + str(ct) + '.csv'\n",
        "progressfull_path = '/home/kchang/Downloads/MingleNet/ProgressFull/' + dataset_type + '_progress_' + model_type + '_' + str(ct) + '.txt'\n",
        "plot_path = '/home/kchang/Downloads/MingleNet/ProgressFull/' + dataset_type + '_progress_plot_' + model_type  + '_' + str(ct) + '.png'\n",
        "model_path = '/home/kchang/Downloads/MingleNet/ModelSaveTensorFlow/' + dataset_type + '_' + model_type + 'newaug'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgRHifxVVViw"
      },
      "outputs": [],
      "source": [
        "from keras.optimizers import AdamW\n",
        "\n",
        "doubleunet_model = double_unet()\n",
        "optimizer = AdamW(\n",
        "    learning_rate = 0.00001, weight_decay = 0.004)\n",
        "loss = 'binary_crossentropy'\n",
        "doubleunet_model.compile(optimizer=optimizer, loss=loss,\n",
        "                     metrics=['accuracy',f1_score,dice_coeff,dice_loss, total_loss, IoU, zero_IoU, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61qcx9iKVViw",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "step = 0\n",
        "learning_rate = 1e-5\n",
        "min_loss_for_saving = 0.2\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "print(\"Learning rate \",learning_rate)\n",
        "#Loop Through Epochs\n",
        "for epoch in range(0, EPOCHS):\n",
        "\n",
        "    print(f'Training, epoch {epoch+1}')\n",
        "\n",
        "    step += 1\n",
        "    #Clear Keras Session\n",
        "    tf.keras.backend.clear_session()\n",
        "    #Create new augmented image instances\n",
        "    image_augmented, mask_augmented = augment_images()\n",
        "    #Train Model\n",
        "    deeplabv3p_model.fit(x=image_augmented, y=mask_augmented, epochs=1, batch_size=8, validation_data=(X_val, y_val), verbose=1)\n",
        "\n",
        "    #Evaluate Loss Validation\n",
        "    prediction_valid = deeplabv3p_model.predict(X_val, verbose=0, batch_size=8)\n",
        "    loss_valid = dice_metric_loss(y_val, prediction_valid)\n",
        "    loss_valid = loss_valid.numpy()\n",
        "    print(\"Loss Validation: \" + str(loss_valid))\n",
        "\n",
        "    #Evaluate Loss Test\n",
        "    prediction_test = deeplabv3p_model.predict(X_test, verbose=0, batch_size=8)\n",
        "    loss_test = dice_metric_loss(y_test, prediction_test)\n",
        "    loss_test = loss_test.numpy()\n",
        "    print(\"Loss Test: \" + str(loss_test))\n",
        "\n",
        "    #min_loss = 0.2 if the mode validation loss surpassed the minimum save the model\n",
        "    if min_loss_for_saving > loss_valid:\n",
        "        min_loss_for_saving = loss_valid\n",
        "        print(\"Saved model with val_loss: \", loss_valid)\n",
        "        deeplabv3p_model.save(model_path)\n",
        "\n",
        "    #Delete the augmented image instances\n",
        "    del image_augmented\n",
        "    del mask_augmented\n",
        "    tf.compat.v1.reset_default_graph()\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "hours, rem = divmod(elapsed_time, 3600)\n",
        "minutes, seconds = divmod(rem, 60)\n",
        "print(f\"Elapsed Time: {int(hours)}h {int(minutes)}m {int(seconds)}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_N-l_ATAVViw"
      },
      "outputs": [],
      "source": [
        "del doubleunet_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf9WKIPGVViw"
      },
      "source": [
        "## Load Layer 1 Deeplabv3+ Model and Print the evaluation metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URSQwUpiVViw"
      },
      "outputs": [],
      "source": [
        "print(\"Loading the model\")\n",
        "\n",
        "deeplabv3p_model = tf.keras.models.load_model(\"/home/kchang/Downloads/MingleNet/ModelSaveTensorFlow/kvasir_deeplabv3pnewaug\", custom_objects={'f1_score': f1_score,\n",
        "    'dice_coeff': dice_coeff,\n",
        "    'dice_loss': dice_loss,\n",
        "    'total_loss': total_loss,\n",
        "    'IoU': IoU,\n",
        "    'zero_IoU': zero_IoU,\n",
        "    'dice_metric_loss':dice_metric_loss})\n",
        "deeplabv3p_preds_tr = deeplabv3p_model.predict(X_train, batch_size=4)\n",
        "deeplabv3p_preds_val = deeplabv3p_model.predict(X_val, batch_size=4)\n",
        "deeplabv3p_preds_t = deeplabv3p_model.predict(X_test, batch_size=4)\n",
        "del deeplabv3p_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xH5sO7nVViw"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, jaccard_score\n",
        "\n",
        "ground_truth_flat = y_test.flatten()\n",
        "predictions_flat = np.round(deeplabv3p_preds_t).flatten()\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(ground_truth_flat, predictions_flat)\n",
        "precision = precision_score(ground_truth_flat, predictions_flat)\n",
        "recall = recall_score(ground_truth_flat, predictions_flat)\n",
        "f1 = f1_score(ground_truth_flat, predictions_flat)\n",
        "jaccard = jaccard_score(ground_truth_flat, predictions_flat)\n",
        "\n",
        "# Print or log the evaluation metrics\n",
        "print(\"Deeplab V3 Plus 1: \")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1}\")\n",
        "print(f\"IoU: {jaccard}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGwyMK1kVViw"
      },
      "source": [
        "## Load Layer 1 DoubleUNet Model and Print the evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhTW2l0rVViw"
      },
      "outputs": [],
      "source": [
        "print(\"Loading the model\")\n",
        "\n",
        "doubleunet_model = tf.keras.models.load_model(\"/home/kchang/Downloads/MingleNet/ModelSaveTensorFlow/kvasir_doubleunetnewaug\", custom_objects={'f1_score': f1_score,\n",
        "    'dice_coeff': dice_coeff,\n",
        "    'dice_loss': dice_loss,\n",
        "    'total_loss': total_loss,\n",
        "    'IoU': IoU,\n",
        "    'zero_IoU': zero_IoU,\n",
        "    'dice_metric_loss':dice_metric_loss})\n",
        "doubleunet_preds_tr = doubleunet_model.predict(X_train, batch_size=4)\n",
        "doubleunet_preds_val = doubleunet_model.predict(X_val, batch_size=4)\n",
        "doubleunet_preds_t = doubleunet_model.predict(X_test, batch_size=4)\n",
        "del doubleunet_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBuh8CrMVViw"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, jaccard_score\n",
        "\n",
        "ground_truth_flat = y_test.flatten()\n",
        "predictions_flat = np.round(doubleunet_preds_t).flatten()\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(ground_truth_flat, predictions_flat)\n",
        "precision = precision_score(ground_truth_flat, predictions_flat)\n",
        "recall = recall_score(ground_truth_flat, predictions_flat)\n",
        "f1 = f1_score(ground_truth_flat, predictions_flat)\n",
        "jaccard = jaccard_score(ground_truth_flat, predictions_flat)\n",
        "\n",
        "# Print or log the evaluation metrics\n",
        "print(\"Double U-Net 1: \")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1}\")\n",
        "print(f\"IoU: {jaccard}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yCD2zrLVVix"
      },
      "source": [
        "## Average Layer 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NP9UdI2RVVix"
      },
      "outputs": [],
      "source": [
        "#Combine DoubleUnet and DeepLabv3+ Train predictions using averaging method and concatenate it with the training set.\n",
        "average_preds_tr = (doubleunet_preds_tr + deeplabv3p_preds_tr) / 2.0\n",
        "train_with_predictions = np.concatenate([X_train, average_preds_tr], axis=-1)\n",
        "\n",
        "#Combine DoubleUnet and DeepLabv3+ Validation Predictions using averaging method and concatenate it with the validation set.\n",
        "average_preds_val = (doubleunet_preds_val + deeplabv3p_preds_val) / 2.0\n",
        "val_with_predictions = np.concatenate([X_val, average_preds_val], axis=-1)\n",
        "\n",
        "#Combine DoubleUnet and DeepLabv3+ Test Predictions using averaging method and concatenate it with the validation set.\n",
        "average_preds_t = (doubleunet_preds_t + deeplabv3p_preds_t) / 2.0\n",
        "test_with_predictions = np.concatenate([X_test, average_preds_t], axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjfCHWWGVVix"
      },
      "source": [
        "# Layer 2 (U-Net and Deeplabv1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZJqLNAyVVix"
      },
      "source": [
        "## Train U-Net Layer 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpRsXdH6VVix"
      },
      "outputs": [],
      "source": [
        "def f1_score(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(tf.math.round(y_pred), tf.float32)\n",
        "\n",
        "    tp = tf.reduce_sum(y_true * y_pred)\n",
        "    fp = tf.reduce_sum((1 - y_true) * y_pred)\n",
        "    fn = tf.reduce_sum(y_true * (1 - y_pred))\n",
        "\n",
        "    precision = tp / (tp + fp + tf.keras.backend.epsilon())\n",
        "    recall = tp / (tp + fn + tf.keras.backend.epsilon())\n",
        "\n",
        "    f1 = 2 * precision * recall / (precision + recall + tf.keras.backend.epsilon())\n",
        "    return f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBn9Ga5_VVix"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "log_dir = './logs/fit' + datetime.now().strftime('%d.%m.%Y--%H-%M-%S')\n",
        "model_path = '/home/kchang/Downloads/ModelSaveTensorFlow/kvasir_unet2_newaug.h5'\n",
        "checkpoint = ModelCheckpoint(model_path, monitor='val_dice_metric_loss', verbose = 1, save_best_only=True,\n",
        "                            mode='min', save_freq='epoch')\n",
        "early = EarlyStopping(monitor='val_dice_metric_loss', min_delta=0, patience = 25, verbose = 1, mode='min')\n",
        "board = TensorBoard(log_dir=log_dir,histogram_freq = 1)\n",
        "tensorboard_callback = [checkpoint,early,board]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7QKOEAXVVix",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(learning_rate = 0.00001, weight_decay = 0.004)\n",
        "loss = 'binary_crossentropy'\n",
        "unet_model2 = unet(input_shape=(352,352,4))\n",
        "unet_model2.compile(optimizer=optimizer, loss=loss, metrics=['accuracy',f1_score,dice_metric_loss,dice_coeff,dice_loss, total_loss, IoU, zero_IoU, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "unet_model2.fit(train_with_predictions, y_train,\n",
        "                  batch_size=8,epochs=400,\n",
        "                  validation_data=(val_with_predictions, y_val),\n",
        "                  callbacks=[tensorboard_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iSrtSFlVVix"
      },
      "outputs": [],
      "source": [
        "del unet_model2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tffCdYTIVVi1"
      },
      "source": [
        "## Train Deeplabv1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbcTvYO4VVi1"
      },
      "outputs": [],
      "source": [
        "def deeplabv1(input_shape=(256,256,3)):\n",
        "\n",
        "    atrous_rates = [6, 12, 18, 24]\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
        "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
        "\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
        "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
        "\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
        "    pool3 = MaxPooling2D((2, 2))(conv3)\n",
        "\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "\n",
        "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
        "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
        "\n",
        "    fusion_layer_deeplab = pool3\n",
        "\n",
        "    # Atrous (dilated) Convolutions with Different Rates\n",
        "    atrous_layers = []\n",
        "    for rate in atrous_rates:\n",
        "        atrous_layer = Conv2D(512, 3, activation='relu', padding='same', dilation_rate=rate)(conv5)\n",
        "        atrous_layers.append(atrous_layer)\n",
        "\n",
        "    # Concatenated Atrous Layers\n",
        "    concat = Concatenate(axis=-1)(atrous_layers)\n",
        "\n",
        "    # Decoder\n",
        "    conv6 = Conv2D(512, 1, activation='relu', padding='same')(concat)\n",
        "\n",
        "    upsample = Conv2DTranspose(1, kernel_size=16, strides=16, padding='same')(conv6)\n",
        "    outputs = Conv2D(1, 1, activation='sigmoid')(upsample)\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQj687EIVVi1"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "log_dir = './logs/fit' + datetime.now().strftime('%d.%m.%Y--%H-%M-%S')\n",
        "model_path = '/home/kchang/Downloads/ModelSaveTensorFlow/kvasir_deeplab2_newaug.h5'\n",
        "checkpoint = ModelCheckpoint(model_path, monitor='val_dice_metric_loss', verbose = 1, save_best_only=True,\n",
        "                            mode='min', save_freq='epoch')\n",
        "early = EarlyStopping(monitor='val_dice_metric_loss', min_delta=0, patience = 25, verbose = 1, mode='min')\n",
        "board = TensorBoard(log_dir=log_dir,histogram_freq = 1)\n",
        "tensorboard_callback = [checkpoint,early,board]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqrSr_0OVVi1",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(learning_rate = 0.00001, weight_decay = 0.004)\n",
        "loss = 'binary_crossentropy'\n",
        "deeplab_model2 = deeplabv1(input_shape=(352,352,4))\n",
        "deeplab_model2.compile(optimizer=optimizer, loss=loss, metrics=['accuracy',f1_score,dice_metric_loss,dice_coeff,dice_loss, total_loss, IoU, zero_IoU, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "deeplab_model2.fit(train_with_predictions, y_train,\n",
        "                  batch_size=8,epochs=400,\n",
        "                  validation_data=(val_with_predictions, y_val),\n",
        "                  callbacks=[tensorboard_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNmF6kkzVVi1"
      },
      "source": [
        "## Load U-Net Layer 2 and print the evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwFI0H6-VVi1"
      },
      "outputs": [],
      "source": [
        "unet2_model = tf.keras.models.load_model('/home/kchang/Downloads/ModelSaveTensorFlow/kvasir_unet2_newaug.h5', custom_objects={'f1_score': f1_score,\n",
        "        'dice_coeff': dice_coeff,\n",
        "        'dice_loss': dice_loss,\n",
        "        'total_loss': total_loss,\n",
        "        'IoU': IoU,\n",
        "        'zero_IoU': zero_IoU,\n",
        "        'dice_metric_loss':dice_metric_loss})\n",
        "unet2_preds_t = unet2_model.predict(test_with_predictions,batch_size=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNRIDx3bVVi1"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, jaccard_score\n",
        "\n",
        "ground_truth_flat = y_test.flatten()\n",
        "predictions_flat = np.round(unet2_preds_t).flatten()\n",
        "\n",
        "accuracy = accuracy_score(ground_truth_flat, predictions_flat)\n",
        "precision = precision_score(ground_truth_flat, predictions_flat)\n",
        "recall = recall_score(ground_truth_flat, predictions_flat)\n",
        "f1 = f1_score(ground_truth_flat, predictions_flat)\n",
        "jaccard = jaccard_score(ground_truth_flat, predictions_flat)\n",
        "\n",
        "print(\"U-Net 2: \")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1}\")\n",
        "print(f\"IoU: {jaccard}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Deeplab Layer 2 and print the evaluation metrics"
      ],
      "metadata": {
        "id": "SAsWupQ5MEOs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUF3oKdmVVi1"
      },
      "outputs": [],
      "source": [
        "deeplab2_model = tf.keras.models.load_model(\"/home/kchang/Downloads/ModelSaveTensorFlow/kvasir_deeplab2_newaug.h5\", custom_objects={'f1_score': f1_score,\n",
        "    'dice_coeff': dice_coeff,\n",
        "    'dice_loss': dice_loss,\n",
        "    'total_loss': total_loss,\n",
        "    'IoU': IoU,\n",
        "    'zero_IoU': zero_IoU,\n",
        "    'dice_metric_loss':dice_metric_loss})\n",
        "deeplab2_preds_t = deeplab2_model.predict(test_with_predictions,batch_size=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1lU1sdrVVi1"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, jaccard_score\n",
        "\n",
        "ground_truth_flat = y_test.flatten()\n",
        "predictions_flat = np.round(deeplab2_preds_t).flatten()\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(ground_truth_flat, predictions_flat)\n",
        "precision = precision_score(ground_truth_flat, predictions_flat)\n",
        "recall = recall_score(ground_truth_flat, predictions_flat)\n",
        "f1 = f1_score(ground_truth_flat, predictions_flat)\n",
        "jaccard = jaccard_score(ground_truth_flat, predictions_flat)\n",
        "\n",
        "# Print or log the evaluation metrics\n",
        "print(\"Deeplab 2: \")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1}\")\n",
        "print(f\"IoU: {jaccard}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRjoK7UjVVi1"
      },
      "source": [
        "# Average Layer 2 prediction and print the evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ds7uddpLVVi1"
      },
      "outputs": [],
      "source": [
        "#Combine UNet and DeepLab preds by using the averaging method and we get the final prediction.\n",
        "average2_preds_test = (unet2_preds_t + deeplab2_preds_t) / 2.0\n",
        "average2_preds_t = np.round(average_preds_test).flatten()\n",
        "y_test_flat = y_test.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UngUfjRPVVi1"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, jaccard_score\n",
        "avg_ensemble_accuracy = accuracy_score(y_test_flat, average2_preds_t)\n",
        "avg_ensemble_f1 = f1_score(y_test_flat, average2_preds_t)\n",
        "avg_ensemble_precision = precision_score(y_test_flat, average2_preds_t)\n",
        "avg_ensemble_recall = recall_score(y_test_flat, average2_preds_t)\n",
        "avg_ensemble_iou = jaccard_score(y_test_flat, average2_preds_t)\n",
        "\n",
        "# Display ensemble metrics\n",
        "print(\"Averaged Ensemble Accuracy:\", avg_ensemble_accuracy)\n",
        "print(\"Averaged Ensemble F1 Score:\", avg_ensemble_f1)\n",
        "print(\"Averaged Ensemble Precision:\", avg_ensemble_precision)\n",
        "print(\"Averaged Ensemble Recall:\", avg_ensemble_recall)\n",
        "print(\"Averaged Ensemble IoU:\", avg_ensemble_iou)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC4Xxxq8VVi1"
      },
      "source": [
        "# Visualization of Models from layer 1, layer 2, and final output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Pyb4msDbZMR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "num_samples_to_visualize = 20\n",
        "random_indices = np.random.choice(range(len(X_test)), num_samples_to_visualize, replace=False)\n",
        "\n",
        "# Visualize each selected sample\n",
        "for sample_index in random_indices:\n",
        "    # Load the original image and corresponding mask\n",
        "    original_image = X_test[sample_index]\n",
        "    actual_mask = y_test[sample_index]\n",
        "\n",
        "    # Get predictions for the chosen sample\n",
        "    doubleunet_prediction = np.round(doubleunet_preds_t[sample_index, ..., 0])\n",
        "    deeplabv3p_prediction = np.round(deeplabv3p_preds_t[sample_index, ..., 0])\n",
        "    avg_prediction = np.round(average_preds_test[sample_index])\n",
        "    unet2_prediction = np.round(unet2_preds_t[sample_index, ..., 0])\n",
        "    deeplab2_prediction = np.round(deeplab2_preds_t[sample_index, ..., 0])\n",
        "    avg2_prediction = np.round(average2_preds_test[sample_index])\n",
        "    # Plot the images and predictions\n",
        "    plt.figure(figsize=(20, 20))\n",
        "\n",
        "    plt.subplot(1, 11, 1)\n",
        "    plt.imshow(original_image)\n",
        "    plt.title(\"Original Image\")\n",
        "\n",
        "    plt.subplot(1, 11, 2)\n",
        "    plt.imshow(original_image[:, :, 0], cmap='Reds')\n",
        "    plt.title(\"Red Channel\")\n",
        "\n",
        "    plt.subplot(1, 11, 3)\n",
        "    plt.imshow(original_image[:, :, 1], cmap='Greens')\n",
        "    plt.title(\"Green Channel\")\n",
        "\n",
        "    plt.subplot(1, 11, 4)\n",
        "    plt.imshow(original_image[:, :, 2], cmap='Blues')\n",
        "    plt.title(\"Blue Channel\")\n",
        "\n",
        "    plt.subplot(1, 11, 5)\n",
        "    plt.imshow(actual_mask, cmap='gray')\n",
        "    plt.title(\"Actual Mask\")\n",
        "\n",
        "    plt.subplot(1, 11, 6)\n",
        "    plt.imshow(doubleunet_prediction, cmap='gray')\n",
        "    plt.title(\"DoubleU-Net\")\n",
        "\n",
        "    plt.subplot(1, 11, 7)\n",
        "    plt.imshow(deeplabv3p_prediction, cmap='gray')\n",
        "    plt.title(\"Deeplab V3 Plus\")\n",
        "\n",
        "    plt.subplot(1, 11, 8)\n",
        "    plt.imshow(avg_prediction, cmap='gray')\n",
        "    plt.title(\"Avg Layer 1\")\n",
        "\n",
        "    plt.subplot(1, 11, 9)\n",
        "    plt.imshow(unet2_prediction, cmap='gray')\n",
        "    plt.title(\"U-Net 2\")\n",
        "\n",
        "    plt.subplot(1, 11, 10)\n",
        "    plt.imshow(deeplab2_prediction, cmap='gray')\n",
        "    plt.title(\"Deeplab 2\")\n",
        "\n",
        "    plt.subplot(1, 11, 11)\n",
        "    plt.imshow(avg2_prediction, cmap='gray')\n",
        "    plt.title(\"Avg Layer 2\")\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}